{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "pb1hxLve28rw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from linformer import Linformer\n",
    "from PIL import Image\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm.notebook import tqdm\n",
    "from vit_pytorch.efficient import ViT\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to determine the device handle for GPU 0000:B1:00.0: Unknown Error\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "mZeP2kIh3x4g"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "batch_size = 8\n",
    "epochs = 20\n",
    "lr = 3e-5\n",
    "gamma = 0.7\n",
    "seed = 142\n",
    "IMG_SIZE = 512\n",
    "patch_size = 16\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "rfBVKh4h36dh"
   },
   "outputs": [],
   "source": [
    "# Tensor Transforms (with Augmentation) and Pytorch Preprocessing:\n",
    "train_ds = torchvision.datasets.ImageFolder(\"E:\\\\JK\\\\Vt\\\\dataset_vit\\\\train\", transform=ToTensor())\n",
    "valid_ds = torchvision.datasets.ImageFolder(\"E:\\\\JK\\\\Vt\\dataset_vit\\\\val\", transform=ToTensor())\n",
    "test_ds = torchvision.datasets.ImageFolder(\"E:\\\\JK\\\\Vt\\\\dataset_vit\\\\test\", transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "3sPVGxex4NFY"
   },
   "outputs": [],
   "source": [
    "# Data Loaders:\n",
    "train_loader = data.DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=4)\n",
    "valid_loader = data.DataLoader(valid_ds, batch_size=batch_size, shuffle=True,  num_workers=4)\n",
    "test_loader  = data.DataLoader(test_ds, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "dwEp0t144lRN"
   },
   "outputs": [],
   "source": [
    "# Training device:\n",
    "device = 'cuda'\n",
    "\n",
    "# Linear Transformer:\n",
    "efficient_transformer = Linformer(dim=512, seq_len=64+1, depth=12, heads=8, k=64)\n",
    "\n",
    "# Vision Transformer Model:\n",
    "model = ViT(dim=512, image_size=512, patch_size=patch_size, num_classes=num_classes, transformer=efficient_transformer, channels=3).to(device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Learning Rate Scheduler for Optimizer:\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "OljvLwdR4p--"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd14a9a48ed457bb70f61551a1ccfd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "the sequence length of the key / values must be 65 - 1025 given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 9\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, label)\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\vit_pytorch\\efficient.py:44\u001b[0m, in \u001b[0;36mViT.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     42\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((cls_tokens, x), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     43\u001b[0m x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding[:, :(n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m---> 44\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     48\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_latent(x)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\linformer\\linformer.py:154\u001b[0m, in \u001b[0;36mLinformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\linformer\\reversible.py:149\u001b[0m, in \u001b[0;36mSequentialSequence.forward\u001b[1;34m(self, x, **kwargs)\u001b[0m\n\u001b[0;32m    146\u001b[0m     layers_and_args \u001b[38;5;241m=\u001b[39m layer_drop(layers_and_args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_dropout)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (f, g), (f_args, g_args) \u001b[38;5;129;01min\u001b[39;00m layers_and_args:\n\u001b[1;32m--> 149\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m f(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mf_args)\n\u001b[0;32m    150\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m g(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mg_args)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\linformer\\linformer.py:35\u001b[0m, in \u001b[0;36mPreNorm.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     34\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\linformer\\linformer.py:97\u001b[0m, in \u001b[0;36mLinformerSelfAttention.forward\u001b[1;34m(self, x, context, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m b, n, d, d_h, h, k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m*\u001b[39mx\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_head, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk\n\u001b[0;32m     96\u001b[0m kv_len \u001b[38;5;241m=\u001b[39m n \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m context\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m kv_len \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe sequence length of the key / values must be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkv_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m given\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     99\u001b[0m queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_q(x)\n\u001b[0;32m    101\u001b[0m proj_seq_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m args: torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbnd,nk->bkd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[1;31mAssertionError\u001b[0m: the sequence length of the key / values must be 65 - 1025 given"
     ]
    }
   ],
   "source": [
    "# Training:\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    for data, label in tqdm(train_loader):\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = (output.argmax(dim=1) == label).float().mean()\n",
    "        epoch_accuracy += acc / len(train_loader)\n",
    "        epoch_loss += loss / len(train_loader)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            epoch_val_accuracy = 0\n",
    "            epoch_val_loss = 0\n",
    "\n",
    "        for data, label in valid_loader:\n",
    "\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            val_output = model(data)\n",
    "            val_loss = criterion(val_output, label)\n",
    "\n",
    "            acc = (val_output.argmax(dim=1) == label).float().mean()\n",
    "            epoch_val_accuracy += acc / len(valid_loader)\n",
    "            epoch_val_loss += val_loss / len(valid_loader)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "y5zjyVTJ45JF"
   },
   "outputs": [],
   "source": [
    "# Save Model:\n",
    "PATH = \"epochs\"+\"_\"+str(epochs)+\"_\"+\"img\"+\"_\"+str(IMG_SIZE)+\"_\"+\"patch\"+\"_\"+str(patch_size)+\"_\"+\"lr\"+\"_\"+str(lr)+\".pt\"\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "zgt_KVcs494V"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ViT:\n\tsize mismatch for pos_embedding: copying a param with shape torch.Size([1, 65, 128]) from checkpoint, the shape in current model is torch.Size([1, 50, 128]).\n\tsize mismatch for to_patch_embedding.1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for to_patch_embedding.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for to_patch_embedding.2.weight: copying a param with shape torch.Size([128, 768]) from checkpoint, the shape in current model is torch.Size([128, 3072]).\n\tsize mismatch for transformer.net.layers.0.0.fn.proj_k: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.0.0.fn.proj_v: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.1.0.fn.proj_k: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.1.0.fn.proj_v: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.2.0.fn.proj_k: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.2.0.fn.proj_v: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.3.0.fn.proj_k: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.3.0.fn.proj_v: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.4.0.fn.proj_k: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.4.0.fn.proj_v: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.5.0.fn.proj_k: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.5.0.fn.proj_v: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.6.0.fn.proj_k: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.6.0.fn.proj_v: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.7.0.fn.proj_k: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.7.0.fn.proj_v: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.8.0.fn.proj_k: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.8.0.fn.proj_v: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.9.0.fn.proj_k: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.9.0.fn.proj_v: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.10.0.fn.proj_k: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.10.0.fn.proj_v: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.11.0.fn.proj_k: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.11.0.fn.proj_v: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m efficient_transformer \u001b[38;5;241m=\u001b[39m Linformer(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m49\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m ViT(image_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m224\u001b[39m, patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m ,transformer\u001b[38;5;241m=\u001b[39mefficient_transformer, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1482\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1477\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   1478\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1479\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   1481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1483\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   1484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ViT:\n\tsize mismatch for pos_embedding: copying a param with shape torch.Size([1, 65, 128]) from checkpoint, the shape in current model is torch.Size([1, 50, 128]).\n\tsize mismatch for to_patch_embedding.1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for to_patch_embedding.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for to_patch_embedding.2.weight: copying a param with shape torch.Size([128, 768]) from checkpoint, the shape in current model is torch.Size([128, 3072]).\n\tsize mismatch for transformer.net.layers.0.0.fn.proj_k: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.0.0.fn.proj_v: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.1.0.fn.proj_k: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.1.0.fn.proj_v: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.2.0.fn.proj_k: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.2.0.fn.proj_v: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.3.0.fn.proj_k: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.3.0.fn.proj_v: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.4.0.fn.proj_k: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.4.0.fn.proj_v: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.5.0.fn.proj_k: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.5.0.fn.proj_v: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.6.0.fn.proj_k: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.6.0.fn.proj_v: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.7.0.fn.proj_k: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.7.0.fn.proj_v: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.8.0.fn.proj_k: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.8.0.fn.proj_v: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.9.0.fn.proj_k: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.9.0.fn.proj_v: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.10.0.fn.proj_k: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.10.0.fn.proj_v: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.11.0.fn.proj_k: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).\n\tsize mismatch for transformer.net.layers.11.0.fn.proj_v: copying a param with shape torch.Size([65, 64]) from checkpoint, the shape in current model is torch.Size([50, 64])."
     ]
    }
   ],
   "source": [
    "# load saved model:\n",
    "#PATH = \"epochs\"+\"_\"+str(epochs)+\"_\"+\"img\"+\"_\"+str(IMG_SIZE)+\"_\"+\"patch\"+\"_\"+str(patch_size)+\"_\"+\"lr\"+\"_\"+str(lr)+\".pt\"\n",
    "PATH='E:\\\\JK\\\\Vt\\\\epochs_10_img_128_patch_16_lr_3e-05.pt'\n",
    "efficient_transformer = Linformer(dim=128, seq_len=49+1, depth=12, heads=8, k=64)\n",
    "model = ViT(image_size=224, patch_size=32, num_classes=2, dim=128 ,transformer=efficient_transformer, channels=3)\n",
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "yPYXM-TZ5CZ5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214d51f2a21b456ab18bcdb651bdf443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper__native_layer_norm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m     y_truth_out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mfloat\u001b[39m(y_truth[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_truth))])\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m test_loss, accuracy, y_proba_out, y_truth_out\n\u001b[1;32m---> 43\u001b[0m loss, acc, y_proba, y_truth \u001b[38;5;241m=\u001b[39m \u001b[43moverall_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(pd\u001b[38;5;241m.\u001b[39mvalue_counts(y_truth))\n",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36moverall_accuracy\u001b[1;34m(model, test_loader, criterion)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m tqdm(test_loader):\n\u001b[0;32m     25\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m), data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m criterion(output, y\u001b[38;5;241m.\u001b[39mlong())\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(output):\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\vit_pytorch\\efficient.py:38\u001b[0m, in \u001b[0;36mViT.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m---> 38\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_patch_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     b, n, _ \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     41\u001b[0m     cls_tokens \u001b[38;5;241m=\u001b[39m repeat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_token, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m() n d -> b n d\u001b[39m\u001b[38;5;124m'\u001b[39m, b \u001b[38;5;241m=\u001b[39m b)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\normalization.py:189\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\functional.py:2347\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2345\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[0;32m   2346\u001b[0m     )\n\u001b[1;32m-> 2347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper__native_layer_norm)"
     ]
    }
   ],
   "source": [
    "# Performance on Valid/Test Data\n",
    "def overall_accuracy(model, test_loader, criterion):\n",
    "\n",
    "    '''\n",
    "    Model testing\n",
    "\n",
    "    Args:\n",
    "        model: model used during training and validation\n",
    "        test_loader: data loader object containing testing data\n",
    "        criterion: loss function used\n",
    "\n",
    "    Returns:\n",
    "        test_loss: calculated loss during testing\n",
    "        accuracy: calculated accuracy during testing\n",
    "        y_proba: predicted class probabilities\n",
    "        y_truth: ground truth of testing data\n",
    "    '''\n",
    "\n",
    "    y_proba = []\n",
    "    y_truth = []\n",
    "    test_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for data in tqdm(test_loader):\n",
    "        X, y = data[0].to('cpu'), data[1].to('cpu')\n",
    "        output = model(X)\n",
    "        test_loss += criterion(output, y.long()).item()\n",
    "        for index, i in enumerate(output):\n",
    "            y_proba.append(i[1])\n",
    "            y_truth.append(y[index])\n",
    "            if torch.argmax(i) == y[index]:\n",
    "                correct+=1\n",
    "            total+=1\n",
    "\n",
    "    accuracy = correct/total\n",
    "\n",
    "    y_proba_out = np.array([float(y_proba[i]) for i in range(len(y_proba))])\n",
    "    y_truth_out = np.array([float(y_truth[i]) for i in range(len(y_truth))])\n",
    "\n",
    "    return test_loss, accuracy, y_proba_out, y_truth_out\n",
    "\n",
    "\n",
    "loss, acc, y_proba, y_truth = overall_accuracy(model, test_loader, criterion = nn.CrossEntropyLoss())\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {acc}\")\n",
    "\n",
    "print(pd.value_counts(y_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lN3M-dIE5H8d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot ROC curve:\n",
    "\n",
    "def plot_ROCAUC_curve(y_truth, y_proba, fig_size):\n",
    "\n",
    "    '''\n",
    "    Plots the Receiver Operating Characteristic Curve (ROC) and displays Area Under the Curve (AUC) score.\n",
    "\n",
    "    Args:\n",
    "        y_truth: ground truth for testing data output\n",
    "        y_proba: class probabilties predicted from model\n",
    "        fig_size: size of the output pyplot figure\n",
    "\n",
    "    Returns: void\n",
    "    '''\n",
    "\n",
    "    fpr, tpr, threshold = roc_curve(y_truth, y_proba)\n",
    "    auc_score = roc_auc_score(y_truth, y_proba)\n",
    "    txt_box = \"AUC Score: \" + str(round(auc_score, 4))\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1],'--')\n",
    "    plt.annotate(txt_box, xy=(0.65, 0.05), xycoords='axes fraction')\n",
    "    plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "    plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "    plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "#     plt.savefig('ROC.png')\n",
    "plot_ROCAUC_curve(y_truth, y_proba, (8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Jw0CxB-U5NFD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoIAAAGfCAYAAADLULPNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAbvElEQVR4nO3df7ilZVkv8O/NAOVRxBSQXxOCYIbmSY9AdUSQ/AEpkJIIHhVLGi1I/JHFCS5TDnbKjp7S6NT440L0EKIdE3UMPZrYdDnFpCQCoSOWzAwEqGmZJ2ZmP+ePvRg3wzB7+br3Xnu97+fDtS7W8+5nvfte1wXDzX0/z/tUay0AAAzPbpMOAACAyZAIAgAMlEQQAGCgJIIAAAMlEQQAGCiJIADAQEkEAQCmQFWdWFU3V9WGqjp/Jz9/cVXdWVXXjV5nz3fP3RcnVAAAFkpVrUhySZKnJdmY5Nqquqq1duMOU9/bWjt33PuqCAIALH9HJ9nQWrultXZ3kiuSnPr93nTRK4Jb7rrF0SXAWB5w4LGTDgGYElvv3lSTjmEhc5w9933kS5OsmnNpdWtt9ZzxQUlunTPemOSYndzqtKp6cpIvJnlla+3WnczZTmsYAGDCRknf6nkn7tqHkvxJa+3fq+qlSd6V5IRdfUAiCADQxcy2pfxtm5KsnDM+eHRtu9ba1+YM357kjfPd1BpBAIAu2szCveZ3bZIjqurQqtozyRlJrpo7oaoOmDM8JclN891URRAAYJlrrW2tqnOTXJ1kRZJ3ttZuqKqLkqxvrV2V5OVVdUqSrUm+nuTF8923WlvcvRw2iwDjslkEGNey2Cxy200LluPsccCPTuT7qAgCAHTQxmvpLmvWCAIADJSKIABAFzPTXxGUCAIAdKE1DADAtFIRBADoYmkfKL0oJIIAAF1oDQMAMK1UBAEAurBrGABgmDxQGgCAqaUiCADQhdYwAMBAaQ0DADCtVAQBALrwQGkAgIHSGgYAYFqpCAIAdGHXMADAQGkNAwAwrVQEAQC60BoGABim1qb/8TFawwAAA6UiCADQRQ82i0gEAQC6sEYQAGCgelARtEYQAGCgVAQBALqYmf5dwxJBAIAutIYBAJhWKoIAAF3YNQwAMFBawwAATCsVQQCALrSGAQAGqgeJoNYwAMBAqQgCAHTQmgdKAwAMk9YwAADTSkUQAKCLHjxHUCIIANCF1jAAANNKRRAAoAutYQCAgdIaBgBgWqkIAgB0oTUMADBQWsMAAEwrFUEAgC56UBGUCAIAdNGDNYJawwAAA6UiCADQhdYwAMBAaQ0DADCtVAQBALrQGgYAGCitYQAAppWKIABAF1rDAAAD1YNEUGsYAGCgVAQBALpobdIRfN8kggAAXWgNAwAwrVQEAQC66EFFUCIIANCFB0oDADCtVAQBALroQWtYRRAAoIvWFu41hqo6sapurqoNVXX+LuadVlWtqp443z0lggAAy1xVrUhySZKTkhyZ5MyqOnIn8/ZKcl6Svx7nvhJBAIAuZmYW7jW/o5NsaK3d0lq7O8kVSU7dybz/luR3kvy/cW4qEQQA6GIBE8GqWlVV6+e8Vu3w2w5Kcuuc8cbRte2q6glJVrbWPjLuV7BZBABgwlprq5Os7vr5qtotyZuTvPh7+ZxEEACgi6V9juCmJCvnjA8eXbvHXkkem+RTVZUk+ye5qqpOaa2tv7+bSgQBADpoM+Pt9l0g1yY5oqoOzWwCeEaS52+PpbVvJtnnnnFVfSrJr+4qCUysEQQAWPZaa1uTnJvk6iQ3JbmytXZDVV1UVad0va+KIABAF0v8QOnW2poka3a49tr7mXv8OPeUCAIAdOGsYQAAppWKIABAF0u7WWRRSAQBALpY4jWCi0EiCADQRQ8SQWsEAQAG6nuuCFbVD2X2HLvPL0I8AADToQ1kjeDo6dSnjOb/bZI7quqvWmuvWsTYAACWrwG1hvdurX0ryXOSXNZaOybJUxcvLAAAFtu4ieDuVXVAktOTfHgR46Fn1q5bn2edcXZOOv0X8vZ3X3mfn//ZRz6eY5/5vJx21jk57axz8v6r/nwCUQLL0TOefnxu+MKn8/c3rs2vveacSYcD9zXTFu41IeOuEXx9Zs+2W9tau7aqDkvypcULiz7Ytm1bLn7TJXnb7/1W9t9vnzzv7PPylCcdk0ceesi95p14wnG54NW/PKEogeVot912y1t+/w058WfOzMaNt2XdZ9bkQx/+WG66yX96WEYGdLLIba21x7XWfjlJWmu3JHnz4oVFH1x/0xfzwwcfmJUHHZA99tgjJ/30cfnkX66bdFjAFDj6qMfny1/+h3zlK1/Nli1bcuWVH8wpJz9j0mFB74ybCL51zGuw3R133pX999t3+/jh++2TO+782n3mffyatXn2i34pr7zg4tz2T3cuZYjAMnXgQfvn1o2bt483brotBx64/wQjgp3oe2u4qn4yyU8l2beq5u4QfnCSFbv43Kokq5LkD990cc5+0ZkLECp9dPyTjsnPPO247Lnnnrnyz9bkgovflHe+9bcnHRYAzKv1YNfwfGsE90zyoNG8veZc/1aSn7u/D7XWVidZnSRb7rpl+h+yQyf77btPbr/juxW+f7rjruy378PuNechez94+/vTTn5G3vyH71iy+IDla/Om27Py4AO3jw8+6IBs3nz7BCOCftplIthauybJNVV1aWvtH5coJnrisY9+VL66cXM2br49D9/3YfnoJ67JG3/z1+815867vp5993lokuQv1q7LYYesnESowDJz7frrcvjhh+YRj1iZTZtuz+mnn5oXvsjOYZaZCbZ0F8q4u4b/rap+N8ljkvzgPRdbaycsSlT0wu67r8hvvPKX8tJXXZht27bl2c96eg4/7JD8wdsuy2Me/ag85difyHve98F8au26rNh9Rfbea69cfOGrJx02sAxs27Yt573iwqz5yOVZsdtuufRd782NN35x0mHBvfVg13C1MY5HqaqPJXlvkl9N8rIkZyW5s7X267v8YLSGgfE94MBjJx0CMCW23r2pJh3Dty9+wYLlOA+88D0T+T7j7hp+WGvtHUm2tNauaa39QhLVQABguPq+a3iOLaO/31ZVz0yyOclDFyckAIApMIBdw/e4uKr2TvLqzD4/8MFJXrFYQQEAsPjGbQ0/N7PrCb/QWntKkqclefbihQUAsMwNqDX8uNbaP98zaK19vaoevzghAQBMgR7sGh63IrhbVf3QPYOqemjGTyIBAFiGxk3m3pTkM1X1vtH4uUnesDghAQBMgaE8ULq1dllVrc93HxnznNbajYsXFgDA8jaEs4a3GyV+kj8AgJ6wzg8AoIuhtIYBANhBDxLBcXcNAwDQMyqCAABd9OA5ghJBAIAutIYBAJhWKoIAAB20HlQEJYIAAF30IBHUGgYAGCgVQQCALoZ0xBwAAHNoDQMAMK1UBAEAuuhBRVAiCADQQWvTnwhqDQMADJSKIABAF1rDAAAD1YNEUGsYAGCgVAQBADpw1jAAwFD1IBHUGgYAGCgVQQCALqb/qGGJIABAF31YI6g1DAAwUCqCAABd9KAiKBEEAOiiB2sEtYYBAAZKRRAAoIM+bBaRCAIAdKE1DADAtFIRBADoQGsYAGCoetAalggCAHTQepAIWiMIADBQKoIAAF30oCIoEQQA6EBrGACAqaUiCADQRQ8qghJBAIAOtIYBAJhaEkEAgA7azMK9xlFVJ1bVzVW1oarO38nPX1ZV11fVdVW1tqqOnO+eEkEAgA6WMhGsqhVJLklyUpIjk5y5k0Tv8tbaj7XWfjzJG5O8eb77SgQBAJa/o5NsaK3d0lq7O8kVSU6dO6G19q05wwcmmfcwZJtFAAC6aLVgt6qqVUlWzbm0urW2es74oCS3zhlvTHLMTu5zTpJXJdkzyQnz/V6JIABABwu5a3iU9K2ed+L897kkySVV9fwkFyY5a1fztYYBAJa/TUlWzhkfPLp2f65I8rPz3VRFEACggzazcK3hMVyb5IiqOjSzCeAZSZ4/d0JVHdFa+9Jo+MwkX8o8JIIAAB0s5QOlW2tbq+rcJFcnWZHkna21G6rqoiTrW2tXJTm3qp6aZEuSb2SetnAiEQQAmAqttTVJ1uxw7bVz3p/3vd5TIggA0EFbwF3DkyIRBADowFnDAABMLRVBAIAOlnjX8KKQCAIAdNDmPcBt+dMaBgAYKBVBAIAOtIYBAAaqD4mg1jAAwECpCAIAdNCHzSISQQCADrSGAQCYWiqCAAAdOGsYAGCgnDUMAMDUUhEEAOhgRmsYAGCY+rBGUGsYAGCgVAQBADrow3MEJYIAAB304WQRrWEAgIFSEQQA6EBrGABgoPrw+BitYQCAgVIRBADooA/PEZQIAgB0YNcwAABTS0UQAKCDPmwWkQgCAHTQhzWCWsMAAAOlIggA0EEfNotIBAEAOujDGkGtYQCAgVIRBADooA+bRSSCAAAdaA0DADC1VAQBADrowaZhiSAAQBd9aA1LBAEAOujDZhFrBAEABkpFEACgg5lJB7AAJIIAAB20aA0DADClVAQBADqY6cHzYySCAAAdzGgNAwAwrVQEAQA66MNmEYkgAEAHfXh8jNYwAMBAqQgCAHSgNQwAMFBawwAATC0VQQCADvpQEZQIAgB00Ic1glrDAAADpSIIANDBzPQXBCWCAABdOGsYAICppSIIANBBm3QAC0AiCADQQR8eH6M1DAAwUCqCAAAdzNT0bxaRCAIAdNCHNYJawwAAA6UiCADQQR82i0gEAQA66MPJIlrDAAADpSIIANCBI+YAAAaqLeBrHFV1YlXdXFUbqur8nfz8VVV1Y1V9vqo+UVWHzHdPiSAAwDJXVSuSXJLkpCRHJjmzqo7cYdrnkjyxtfa4JO9P8sb57isRBADoYKYW7jWGo5NsaK3d0lq7O8kVSU6dO6G19hettX8bDdclOXi+m0oEAQA6mFnAV1Wtqqr1c16rdvh1ByW5dc544+ja/XlJko/O9x1sFgEAmLDW2uokqxfiXlX1giRPTHLcfHMlggAAHSzxEXObkqycMz54dO1equqpSS5Iclxr7d/nu6lEEACggyV+oPS1SY6oqkMzmwCekeT5cydU1eOT/HGSE1trd4xzU2sEAQCWudba1iTnJrk6yU1Jrmyt3VBVF1XVKaNpv5vkQUneV1XXVdVV891XRRAAoIOlPmu4tbYmyZodrr12zvunfq/3lAgCAHSw1IngYtAaBgAYKBVBAIAO2vQfNSwRBADoQmsYAICppSIIANBBHyqCEkEAgA6W+GSRRaE1DAAwUCqCAAAdLPERc4tCIggA0EEf1ghqDQMADJSKIABAB32oCEoEAQA6sGsYAICppSIIANCBXcMAAANljSAAwEBZIwgAwNRSEQQA6GCmBzVBiSAAQAd9WCOoNQwAMFAqggAAHUx/Y1giCADQidYwAABTS0UQAKADJ4sAAAxUHx4fozUMADBQKoIAAB1Mfz1QIggA0IldwwAATC0VQQCADvqwWUQiCADQwfSngVrDAACDpSIIANBBHzaLSAQBADrowxpBrWEAgIFSEQQA6GD664ESQQCATvqwRlBrGABgoFQEAQA6aD1oDksEAQA60BoGAGBqjZUIVtUjq+oHRu+Pr6qXV9VDFjUyAIBlbCZtwV6TMm5F8E+TbKuqw5OsTrIyyeWLFhUAwDLXFvA1KeMmgjOtta1Jnp3kra211yQ5YPHCAgBgsY27WWRLVZ2Z5KwkJ4+u7bE4IQEALH9DOmLu55P8ZJI3tNa+UlWHJnn34oUFALC8zSzga1LGSgRbaze21l7eWvuT0fgrrbXfWdzQ6IO169bnWWecnZNO/4W8/d1X3ufnf/aRj+fYZz4vp511Tk4765y8/6o/n0CUwHL0jKcfnxu+8On8/Y1r82uvOWfS4UAvjdUarqrrc9+1jN9Msj7Jxa21ry10YEy/bdu25eI3XZK3/d5vZf/99snzzj4vT3nSMXnkoYfca96JJxyXC179yxOKEliOdtttt7zl99+QE3/mzGzceFvWfWZNPvThj+Wmm7406dBguz48UHrc1vBHk3wkyX8ZvT6U2STw9iSXLkpkTL3rb/pifvjgA7PyoAOyxx575KSfPi6f/Mt1kw4LmAJHH/X4fPnL/5CvfOWr2bJlS6688oM55eRnTDosuJc+tIbH3Szy1NbaE+aMr6+qz7bWnlBVL1iMwJh+d9x5V/bfb9/t44fvt0+uv+Hm+8z7+DVrs/7vrs8jVh6UX3v5S3PAw/e9zxxgWA48aP/cunHz9vHGTbfl6KMeP8GIoJ/GrQiuqKqj7xlU1VFJVoyGW3ecXFWrqmp9Va1/+2V/sgBh0lfHP+mYfOz9l+YDl/2v/ORRT8gFF79p0iEBwFjaAv41KeNWBM9O8s6qelCSSvKtJC+pqgcm+e87Tm6trc7sg6ez5a5bpr+BTif77btPbr/jzu3jf7rjruy378PuNechez94+/vTTn5G3vyH71iy+IDla/Om27Py4AO3jw8+6IBs3nz7BCOC+xrMWcOttWtbaz+W5MeT/MfW2uNG177dWrvvVlBI8thHPypf3bg5Gzffni1btuSjn7gmT3nST9xrzp13fX37+79Yuy6HHbJyqcMElqFr11+Xww8/NI94xMrsscceOf30U/OhD39s0mFB74y7a3jvJL+Z5Mmj8TVJLmqtfXMRY2PK7b77ivzGK38pL33Vhdm2bVue/ayn5/DDDskfvO2yPObRj8pTjv2JvOd9H8yn1q7Lit1XZO+99srFF7560mEDy8C2bdty3isuzJqPXJ4Vu+2WS9/13tx44xcnHRbcy0yb/qZntTG+RFX9aZIvJHnX6NILM1sZfM58n9UaBsb1gAOPnXQIwJTYevemmnQMLzjkOQuW47znH//PRL7PuGsEH9laO23O+PVVdd0ixAMAwBIZd9fwd6rqSfcMquo/J/nO4oQEALD8zaQt2GtSxq0IvizJZaO1gknyjSRnLU5IAADLXx9OFtllIlhVr5ozvCzJA0fvv53kqUk+v0hxAQCwyOarCO41+vuPJDkqyQcz+xzBFyT5m0WMCwBgWevDcwR3mQi21l6fJFX16SRPaK39y2j8usyePQwAMEiTXNu3UMbdLPLwJHfPGd89ugYAwJQad7PIZUn+pqo+MBr/bJJLFyMgAIBp0PvNIvdorb2hqj6a5J6nvf58a+1zixcWAMDy1vs1gnO11j6b5LOLGAsAAEto7EQQAIDvGueY3uVu3M0iAADMsdQni1TViVV1c1VtqKrzd/LzJ1fVZ6tqa1X93Dj3lAgCACxzVbUiySVJTkpyZJIzq+rIHaZ9NcmLk1w+7n21hgEAOljizSJHJ9nQWrslSarqiiSnJrnxngmttX8Y/Wzs0FQEAQA6aAv4V1Wtqqr1c16rdvh1ByW5dc544+ja90VFEACgg4U8WaS1tjrJ6gW74ZhUBAEAlr9NSVbOGR88uvZ9UREEAOhgiR8fc22SI6rq0MwmgGckef73e1MVQQCADmYW8DWf1trWJOcmuTrJTUmubK3dUFUXVdUpSVJVR1XVxiTPTfLHVXXDfPdVEQQAmAKttTVJ1uxw7bVz3l+b2Zbx2CSCAAAdtAXcLDIpEkEAgA4WctfwpFgjCAAwUCqCAAAdLPGu4UUhEQQA6EBrGACAqaUiCADQgV3DAAADNdODNYJawwAAA6UiCADQwfTXAyWCAACd2DUMAMDUUhEEAOigDxVBiSAAQAd9OFlEaxgAYKBUBAEAOtAaBgAYqD6cLKI1DAAwUCqCAAAd9GGziEQQAKCDPqwR1BoGABgoFUEAgA60hgEABkprGACAqaUiCADQQR+eIygRBADoYKYHawS1hgEABkpFEACgA61hAICB0hoGAGBqqQgCAHSgNQwAMFBawwAATC0VQQCADrSGAQAGSmsYAICppSIIANCB1jAAwEC1NjPpEL5vWsMAAAOlIggA0MGM1jAAwDA1u4YBAJhWKoIAAB1oDQMADJTWMAAAU0tFEACggz4cMScRBADooA8ni2gNAwAMlIogAEAHfdgsIhEEAOjA42MAAAaqDxVBawQBAAZKRRAAoAOPjwEAGCitYQAAppaKIABAB3YNAwAMlNYwAABTS0UQAKADu4YBAAaq9WCNoNYwAMBAqQgCAHSgNQwAMFB2DQMAMLVUBAEAOujDZhGJIABAB1rDAABMLYkgAEAHrbUFe42jqk6sqpurakNVnb+Tn/9AVb139PO/rqpHzHdPiSAAQAdtAV/zqaoVSS5JclKSI5OcWVVH7jDtJUm+0Vo7PMn/TPI7891XIggAsPwdnWRDa+2W1trdSa5IcuoOc05N8q7R+/cn+emqql3ddNE3i+yxz2G7DIBhqqpVrbXVk46D5WXr3ZsmHQLLkD8vWK623r1pwXKcqlqVZNWcS6t3+Of+oCS3zhlvTHLMDrfZPqe1trWqvpnkYUnuur/fqyLIpKyafwpAEn9eMACttdWttSfOeS3J//xIBAEAlr9NSVbOGR88urbTOVW1e5K9k3xtVzeVCAIALH/XJjmiqg6tqj2TnJHkqh3mXJXkrNH7n0vyyTbPlmQPlGZSrPcBxuXPCwZvtObv3CRXJ1mR5J2ttRuq6qIk61trVyV5R5J3V9WGJF/PbLK4S9WHp2IDAPC90xoGABgoiSAAwEBJBJmYqjq+qn5q0nEAk1VVr6uqX510HDBEEkEm6fgkEkEAmBCJIAuuql5UVZ+vqr+rqndX1cmjw68/V1X/t6oePjoI+2VJXllV11XVsVX13Kr6wuhzn57w1wAWUVVdUFVfrKq1SX5kdO3Hq2rd6M+PD1TVD42uHzW6dl1V/W5VfWF0/TFV9Tej65+vqiMm+JVgKtk1zIKqqsck+UCSn2qt3VVVD83sedr/3FprVXV2kh9trb26ql6X5F9ba/9j9Nnrk5zYWttUVQ9prf3zhL4GsIiq6j8luTSzx2PtnuSzSf4oyYuS/Epr7ZrRIzEe3Fp7xSjx+8XW2meq6reTPKu19tiqemuSda21/z16rtqK1tp3JvKlYEqpCLLQTkjyvtbaXUnSWvt6Zp9+fvUo0XtNksfcz2f/KsmlVfWLmX1GEtBPxyb5QGvt31pr38rsQ3AfmOQhrbVrRnPeleTJVfWQJHu11j4zun75nPt8JslvVNWvJzlEEgjfO4kgS+GtSf6gtfZjSV6a5Ad3Nqm19rIkF2b2eJy/raqHLV2IwLRprV2e5JQk30mypqpOmHBIMHUkgiy0TyZ57j1J3Kg1vHe+ex7iWXPm/kuSve4ZVNUjW2t/3Vp7bZI7c+8zFYH++HSSn62qB1TVXklOTvLtJN+oqmNHc16Y5JrREpF/qapjRte3n5RQVYcluaW19pYkH0zyuKX6AtAXjphjQY2Ou3lDkmuqaluSzyV5XZL3VdU3MpsoHjqa/qEk76+qU5P8SmY3jhyRpJJ8IsnfLXX8wOJrrX22qt6b2X/H78jsGarJ7P8o/lFV/YcktyT5+dH1lyR5W1XNJLkmyTdH109P8sKq2pLk9iS/tURfAXrDZhEAlrWqelBr7V9H789PckBr7bwJhwW9oCIIwHL3zKr6r5n9b9Y/JnnxZMOB/lARBAAYKJtFAAAGSiIIADBQEkEAgIGSCAIADJREEABgoP4/Vhn8s+UfI18AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 864x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "net = model\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "        output = net(inputs) # Feed Network\n",
    "\n",
    "        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "        y_pred.extend(output) # Save Prediction\n",
    "\n",
    "        labels = labels.data.cpu().numpy()\n",
    "        y_true.extend(labels) # Save Truth\n",
    "\n",
    "# constant for classes\n",
    "classes = ('cats', 'dogs')\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix), index = [i for i in classes],\n",
    "                     columns = [i for i in classes])\n",
    "plt.figure(figsize = (12,7))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "# plt.savefig('cm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfBDfmR_5SR0"
   },
   "outputs": [],
   "source": [
    "# Inference on Single Images (cats-dogs):\n",
    "test_image = \"new_cat_image.jpg\"\n",
    "test_image_null = \"new_dog_image.png\"\n",
    "image = Image.open(test_image)\n",
    "image_null = Image.open(test_image_null)\n",
    "\n",
    "# Define tensor transform and apply it:\n",
    "data_transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "image_t = data_transform(image).unsqueeze(0)\n",
    "image_null_t = data_transform(image_null).unsqueeze(0)\n",
    "\n",
    "# Labels:\n",
    "for inputs, labels in test_loader:\n",
    "        labels = labels.data.cpu().numpy()\n",
    "\n",
    "# Prediction:\n",
    "out_cat = model(image_t)\n",
    "out_dog= model(image_null_t)\n",
    "print(\"predicted cat tensor:\", out_cat)\n",
    "print(\"predicted dog tensor:\", out_dog)\n",
    "print(\"\")\n",
    "# Print:\n",
    "if(labels[out_cat.argmax()]== 0):\n",
    "    print(\"smoke\")\n",
    "else:\n",
    "    print(\"else\")\n",
    "\n",
    "# Show Image:\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "# Print:\n",
    "if(labels[out_dog.argmax()]== 0):\n",
    "    print(\"cat\")\n",
    "else:\n",
    "    print(\"dog\")\n",
    "\n",
    "# Show Image Null:\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(image_null)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images resized successfully!\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#E:\\JK\\Vt\\dataset_vit - Copy\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def resize_images(folder_path, new_width, new_height):\n",
    "  \"\"\"Resizes images in a folder and its subfolders to a new resolution.\n",
    "\n",
    "  Args:\n",
    "    folder_path: Path to the folder containing the images.\n",
    "    new_width: The desired width of the resized images.\n",
    "    new_height: The desired height of the resized images.\n",
    "  \"\"\"\n",
    "  for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "    for filename in filenames:\n",
    "      if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "        image_path = os.path.join(dirpath, filename)\n",
    "        image = cv2.imread(image_path)\n",
    "        resized_image = cv2.resize(image, (new_width, new_height))\n",
    "        cv2.imwrite(image_path, resized_image)\n",
    "\n",
    "# Set the folder path and desired resolution\n",
    "folder_path = \"E:\\JK\\Vt\\dataset_vit - Copy\"\n",
    "new_width = 512\n",
    "new_height = 512\n",
    "\n",
    "# Resize images\n",
    "resize_images(folder_path, new_width, new_height)\n",
    "\n",
    "print(\"Images resized successfully!\")\n",
    "print(\"1\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
